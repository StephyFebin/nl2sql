{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89387f17-b85b-4826-8a0a-60e67c5847d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from autogen import GroupChat, GroupChatManager, register_function\n",
    "from typing import Annotated\n",
    "import sqlite3\n",
    "from typing import Annotated\n",
    "import autogen\n",
    "import json\n",
    "import gradio as gr\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b682b60-8b81-4a64-abb7-154c335d1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "global config_list\n",
    "config_list = {\n",
    "    \"model\": \"llama-3.3-70b-versatile\",\n",
    "    \"api_key\": \"#######\",\n",
    "    \"api_type\": \"groq\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "533b2b44-4fa7-47d9-9aba-e75fa77825a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Table: Customers\\nColumns:\\n  - Customer_ID (INTEGER) | Samples: 1, 2, 3\\n  - Customer_Details (VARCHAR(255)) | Samples: America Jaskolski, Ellsworth Paucek, Mrs. Hanna Willms\\nTable: Customer_Policies\\nColumns:\\n  - Policy_ID (INTEGER) | Samples: 119, 141, 143\\n  - Customer_ID (INTEGER) | Samples: 1, 2, 3\\n  - Policy_Type_Code (CHAR(15)) | Samples: Car, Life, Car\\n  - Start_Date (DATE) | Samples: 2018-01-21, 2017-08-21, 2017-06-16\\n  - End_Date (DATE) | Samples: 2017-12-15, 2017-09-29, 2017-12-09\\nTable: Claims\\nColumns:\\n  - Claim_ID (INTEGER) | Samples: 143, 423, 442\\n  - Policy_ID (INTEGER) | Samples: 744, 552, 473\\n  - Date_Claim_Made (DATE) | Samples: 2017-03-11, 2016-08-12, 2017-02-24\\n  - Date_Claim_Settled (DATE) | Samples: 2017-11-03, 2018-01-27, 2018-01-21\\n  - Amount_Claimed (INTEGER) | Samples: 43884, 79134, 70088\\n  - Amount_Settled (INTEGER) | Samples: 1085, 1724, 1189\\nTable: Settlements\\nColumns:\\n  - Settlement_ID (INTEGER) | Samples: 357, 412, 476\\n  - Claim_ID (INTEGER) | Samples: 486, 621, 801\\n  - Date_Claim_Made (DATE) | Samples: 2018-08-07, 2017-08-27, 2016-09-05\\n  - Date_Claim_Settled (DATE) | Samples: 2018-01-16, 2018-02-04, 2018-03-02\\n  - Amount_Claimed (INTEGER) | Samples: 38543, 57669, 30954\\n  - Amount_Settled (INTEGER) | Samples: 1181, 1427, 1805\\n  - Customer_Policy_ID (INTEGER) | Samples: 515, 617, 943\\nTable: Payments\\nColumns:\\n  - Payment_ID (INTEGER) | Samples: 384, 435, 484\\n  - Settlement_ID (INTEGER) | Samples: 516, 476, 516\\n  - Payment_Method_Code (VARCHAR(255)) | Samples: MasterCard, MasterCard, MasterCard\\n  - Date_Payment_Made (DATE) | Samples: 2018-02-16, 2017-05-28, 2017-06-24\\n  - Amount_Payment (INTEGER) | Samples: 241730, 448613, 456098'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fetch_schema(DB_PATH: Annotated[str, \"Path to the Database file\"]) -> Annotated[str, \"Resulting Schema from the DB\"]:\n",
    "    \"\"\"Fetches the schema of the database (table names, columns, types, and sample values).\"\"\"\n",
    "    \n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Get all table names\n",
    "    cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    tables = cursor.fetchall()\n",
    "    \n",
    "    schema = {}\n",
    "    \n",
    "    for table in tables:\n",
    "        table_name = table[0]\n",
    "        \n",
    "        # Get column details\n",
    "        cursor.execute(f\"PRAGMA table_info({table_name});\")\n",
    "        columns = cursor.fetchall()\n",
    "        \n",
    "        # Get sample data (first 3 rows) from the table\n",
    "        cursor.execute(f\"SELECT * FROM {table_name} LIMIT 3;\")\n",
    "        sample_data = cursor.fetchall()\n",
    "        \n",
    "        schema[table_name] = []\n",
    "        for col in columns:\n",
    "            col_name, col_type = col[1], col[2]\n",
    "            # Extract sample values for the column\n",
    "            col_index = columns.index(col)\n",
    "            sample_values = [row[col_index] for row in sample_data] if sample_data else [\"No data\"]\n",
    "            \n",
    "            schema[table_name].append({\n",
    "                \"name\": col_name,\n",
    "                \"type\": col_type,\n",
    "                \"samples\": sample_values\n",
    "            })\n",
    "    \n",
    "    conn.close()\n",
    "\n",
    "    schema_str = \"\\n\".join([\n",
    "        \"Table: {}\\nColumns:\\n{}\".format(\n",
    "            table, \"\\n\".join(\n",
    "                [\"  - {} ({}) | Samples: {}\".format(col[\"name\"], col[\"type\"], \", \".join(map(str, col[\"samples\"]))) \n",
    "                 for col in cols]\n",
    "            )\n",
    "        )\n",
    "        for table, cols in schema.items()\n",
    "    ])\n",
    "    # print('schema_strschema_strschema_str',schema_str)\n",
    "    return schema_str\n",
    "\n",
    "DB_PATH='spider_db/spider_db.db'\n",
    "fetch_schema(DB_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d054c85-9c77-46c8-ac6c-a2ebce994c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_sql_query(sql_query: str, db_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Helper function to validate an SQL query.\n",
    "    Executes the query on the specified database and returns 'success' if valid, otherwise 'failure'.\n",
    "    \n",
    "    Args:\n",
    "        sql_query (str): The SQL query to validate.\n",
    "        db_path (str): Path to the database file.\n",
    "\n",
    "    Returns:\n",
    "        str: 'success' if the query is valid, otherwise 'failure: <error_message>'.\n",
    "    \"\"\"\n",
    "    import sqlite3  # Or use the relevant database connector\n",
    "\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)  # Connect to the provided database\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"EXPLAIN QUERY PLAN \" + sql_query)  # Lightweight validation\n",
    "        conn.close()\n",
    "        return \"success\"\n",
    "    except Exception as e:\n",
    "        return f\"failure: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46a9c664-da9d-4575-ad67-8f1537283b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_and_execute_sql_query(sql_query: Annotated[str, \"Result of SQL query from SQLAgent\"], db_path: Annotated[str, \"Path to the Database\"]) -> Annotated[dict, \"Result of running the query with keys being either success or failure\"]:\n",
    "    \"\"\"\n",
    "    Validates and executes an SQL query on the specified database.\n",
    "    \n",
    "    Args:\n",
    "        sql_query (str): The SQL query to validate and execute.\n",
    "        db_path (str): Path to the database file.\n",
    "\n",
    "    Returns:\n",
    "        str: 'success' if the query is valid but does not return results (e.g., UPDATE).\n",
    "        list: Query results if the query is valid and returns data (e.g., SELECT).\n",
    "        str: 'failure: <error_message>' if the query is invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Validate query using EXPLAIN QUERY PLAN\n",
    "        cursor.execute(\"EXPLAIN QUERY PLAN \" + sql_query)\n",
    "        \n",
    "        # If validation passes, execute the actual query\n",
    "        cursor.execute(sql_query)\n",
    "\n",
    "        if sql_query.strip().lower().startswith(\"select\"):\n",
    "            # Fetch and return results for SELECT queries\n",
    "            results = cursor.fetchall()\n",
    "            conn.close()\n",
    "            return results  # Returns list of tuples\n",
    "\n",
    "        conn.commit()  # Commit changes for INSERT, UPDATE, DELETE\n",
    "        conn.close()\n",
    "        return {\"success\": results}  # No need to return results for non-SELECT queries\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"failure: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0a59ea-17ff-4307-b5e9-eb6024d1c3c2",
   "metadata": {},
   "source": [
    "#### final function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "526b3b03-310c-4208-9995-74d2c0d838ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_function(DB_PATH,user_query):\n",
    "\n",
    "    user_proxy = autogen.ConversableAgent(\n",
    "    name=\"Admin\",\n",
    "    code_execution_config=False,\n",
    "    llm_config=config_list,\n",
    "    human_input_mode=\"NEVER\", max_consecutive_auto_reply=1,\n",
    "    system_message = f\"Analyze the user query :{user_query}, and pass it to SchemaAgent for further process.\",\n",
    "    is_termination_msg=lambda msg: (\n",
    "                                        msg.get(\"content\") is not None \n",
    "                                        and \"thank you\" in msg[\"content\"].lower() \n",
    "                                        and msg[\"content\"].strip().lower() != \"thank you.\"\n",
    "                                    )\n",
    ")\n",
    "\n",
    "\n",
    "    schema_agent = autogen.ConversableAgent(\n",
    "        name=\"SchemaAgent\",\n",
    "        llm_config=config_list,\n",
    "        system_message=f\"As a database expert, your task is to inspect the database located at {DB_PATH} \"\n",
    "            f\"by internally calling the function fetch_schema({DB_PATH}). \"\n",
    "            \"Do not print or output the schema directly. Instead, analyze it to identify the structure, including table names, columns, and data types. \"\n",
    "            f\"Next, examine the user query: {user_query} and determine which tables are most relevant for answering it. \"\n",
    "            \"If no relevant tables can be identified, alert the user with the message: \"\n",
    "            \"'The query cannot be answered as no relevant tables were found in the database.' Provide a reason for this and terminate the chat by saying 'Thank you'. \"\n",
    "            \"Once the relevant tables are determined or the query is deemed unanswerable, terminate the process by saying 'terminate' \"\n",
    "            \"and return only the necessary information to the SQLAgent.\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    sql_agent = autogen.ConversableAgent(\n",
    "        name=\"SQLAgent\",\n",
    "        llm_config=config_list,\n",
    "        system_message=(\n",
    "            \"As a SQL expert, your task is to generate an accurate SQL query based on the provided database schema and user query.and column name should be in the table\"\n",
    "            \"You will receive the structured schema and the user query as input from SchemaAgent.\"\n",
    "            \"Analyze the schema to determine the relevant tables and construct an optimized SQL query that accurately answers the user's request. \"\n",
    "            \"Once you have generated the SQL query, terminate the process by saying 'terminate' and return the final SQL query.\"\n",
    "            \"If no relevant tables can be identified, alert the user with the message: 'The query cannot be answered as no relevant tables were found in the database.' Provide a reason for this and terminate the chat by saying thank you\"\n",
    "            \n",
    "        )\n",
    "    )\n",
    "    \n",
    "    validation_agent = autogen.ConversableAgent(\n",
    "        name=\"ValidationAgent\",\n",
    "        llm_config=config_list,\n",
    "        system_message=(\n",
    "            \"As a SQL validation expert, your task is to validate the SQL query generated by SQLAgent. \"\n",
    "            f\"You will receive an SQL query and the database path : {DB_PATH} as input. \"\n",
    "            f\"Use the provided helper function `validate_and_execute_sql_query(sql_query, {DB_PATH})` to check whether the query is correct. \"\n",
    "            \"If the query is valid, respond with 'SQL query is valid' and return the result to ResultFormattingAgent.\"\n",
    "            \"If the query is invalid, respond with 'SQL query is invalid' along with the error message to SQLErrorCorrectionAgent.\"\n",
    "            \"Once validation is complete, terminate the process by saying 'terminate'.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    sql_error_correction_agent = autogen.ConversableAgent(\n",
    "        name=\"SQLErrorCorrectionAgent\",\n",
    "        llm_config=config_list,\n",
    "        system_message=(\n",
    "            \"You are an expert in SQL query error correction. \"\n",
    "            \"Your task is to correct SQL query errors based on the given failure message and database schema. \"\n",
    "            \"You will receive a failed SQL query along with an error message explaining why it failed. \"\n",
    "            \"Ensure the corrected query follows proper SQL syntax and is aligned with the database schema. \"\n",
    "            \"Once you have corrected the query, return the fixed version to the ValidationAgent for validation.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    " \n",
    "    result_formatting_agent = autogen.ConversableAgent(\n",
    "        name=\"ResultFormattingAgent\",\n",
    "        llm_config=config_list,\n",
    "        system_message=(\n",
    "        \"You are responsible for formatting database query results into a structured, human-readable format. \"\n",
    "        \"When provided with raw query results, process them into a clear tabular format, numbered list, or summary, depending on the result type. \"\n",
    "        \"For large datasets, summarize key trends instead of listing every record. \"\n",
    "        \"Return the formatted output in a way that is easy to understand.\"\n",
    "        \"Once you have the final result, output the result and finally close the conversation by saying thank you\"\n",
    "    )\n",
    ")\n",
    "    register_function(\n",
    "    fetch_schema,\n",
    "    caller=schema_agent,\n",
    "    executor=schema_agent,\n",
    "    name=\"FetchingSchemaSchemaAgent\",\n",
    "    description=\"Get Fetch Schema\",\n",
    "    )\n",
    "    \n",
    "    register_function(\n",
    "        fetch_schema,\n",
    "        caller=user_proxy,\n",
    "        executor=user_proxy,\n",
    "        name=\"FetchingSchemaUserProxy\",\n",
    "        description=\"Get Fetch Schema\",\n",
    "    )\n",
    "    \n",
    "    register_function(\n",
    "        validate_sql_query,\n",
    "        caller=sql_agent,\n",
    "        executor=validation_agent,\n",
    "        name=\"ValidationFromSQLAgent\",\n",
    "        description=\"Validationg the query\",\n",
    "    )\n",
    "    \n",
    "    register_function(\n",
    "        validate_and_execute_sql_query,\n",
    "        caller=validation_agent,\n",
    "        executor=validation_agent,\n",
    "        name=\"ValidationToAgent\",\n",
    "        description=\"Validationg the query\",\n",
    "    )\n",
    "# result_formatting_agent\n",
    "    groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, schema_agent, sql_agent, validation_agent, sql_error_correction_agent, result_formatting_agent],\n",
    "    messages=[],\n",
    "    max_round=5,\n",
    "    allowed_or_disallowed_speaker_transitions={\n",
    "    user_proxy: [schema_agent, sql_agent, validation_agent, sql_error_correction_agent, result_formatting_agent],\n",
    "    schema_agent: [user_proxy, sql_agent],\n",
    "    sql_agent: [user_proxy, validation_agent],\n",
    "    validation_agent: [sql_error_correction_agent, result_formatting_agent],\n",
    "    sql_error_correction_agent: [user_proxy, validation_agent],\n",
    "    result_formatting_agent: [user_proxy]\n",
    "    },\n",
    "    speaker_transitions_type=\"allowed\",\n",
    ")\n",
    "\n",
    "\n",
    "    \n",
    " \n",
    "    \n",
    "    manager = autogen.GroupChatManager(\n",
    "        groupchat=groupchat, llm_config={\"config_list\": config_list},\n",
    "        is_termination_msg=lambda msg: (\n",
    "        isinstance(msg, dict) and\n",
    "        msg.get(\"name\") == \"ResultFormattingAgent\" and\n",
    "        bool(msg.get(\"content\"))  # Ensure content is not empty\n",
    "    )\n",
    "    )\n",
    "    chat_result = user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    max_turns=3,\n",
    "    message = f\"Analyze the user query : {user_query}\"\n",
    "    )\n",
    "\n",
    "    for idx, msg in enumerate(manager.groupchat.messages):\n",
    "        if msg.get(\"name\") == \"SQLAgent\":\n",
    "            for tool_call in msg.get(\"tool_calls\", []):\n",
    "                arguments_str = tool_call.get(\"function\", {}).get(\"arguments\", \"{}\")\n",
    "                arguments = json.loads(arguments_str)  # Convert string to dictionary\n",
    "                global generated_sql\n",
    "                generated_sql = arguments.get(\"sql_query\")\n",
    "                \n",
    "                if generated_sql:  # Only print if the SQL query is not None\n",
    "                    print(\"Extracted SQL Query:\\n\", generated_sql)\n",
    "        print(\"000000000000000000000000000\")\n",
    "        if msg.get('name')=='ValidationAgent':\n",
    "            print('11111111111111111111111111111msg',msg)\n",
    "            if isinstance(msg.get(\"content\"), str) and msg[\"content\"].startswith(\"[('\"):\n",
    "                    generated_result = msg[\"content\"]\n",
    "                    print(\"✅ Found matching message:\\n\", generated_result)\n",
    "        if msg.get('name')=='ResultFormattingAgent':\n",
    "            print('DDDDDDDDDDDDDDDDDDDDDDDDD',msg)\n",
    "             \n",
    "    \n",
    "    judge_agent = autogen.ConversableAgent(\n",
    "        name=\"JudgeAgent\",\n",
    "        llm_config=config_list,\n",
    "        system_message=(\n",
    "            \"You are an expert in SQL query evaluation. Your task is to assess SQL queries generated by the SQLAgent by comparing them with reference SQL queries from a dataset. \"\n",
    "            \"For each comparison, evaluate the following aspects using the defined numeric scoring guidelines:\\n\\n\"\n",
    "    \n",
    "            \"1. Accuracy (0–10): Does the AI-generated SQL return the same results as the reference SQL when executed on the same database?\\n\"\n",
    "            \"- 10: Identical results\\n\"\n",
    "            \"- 5: Partial match (e.g., missing filters, subset/superset of rows)\\n\"\n",
    "            \"- 0: Incorrect or different logic\\n\\n\"\n",
    "    \n",
    "            \"2. Efficiency (0–10): Evaluate the query's performance in terms of execution time, indexing, and join optimization.\\n\"\n",
    "            \"- 10: Highly efficient (<1s, optimized with indexes and joins)\\n\"\n",
    "            \"- 6: Moderate (1–3s, room for improvement)\\n\"\n",
    "            \"- 3: Low efficiency (>3s, full scans or nested loops)\\n\\n\"\n",
    "    \n",
    "            \"3. Hallucination (0–10): Check for invalid elements like non-existent tables, columns, or illogical clauses.\\n\"\n",
    "            \"- 0: No hallucination\\n\"\n",
    "            \"- 5: Minor (1–2 issues)\\n\"\n",
    "            \"- 10: Major hallucinations (e.g., many or critical invalid references)\\n\\n\"\n",
    "    \n",
    "            \"4. Completeness (0–10): Does the query fully answer the user’s question?\\n\"\n",
    "            \"- 10: Fully complete\\n\"\n",
    "            \"- 5: Partially complete (some logic or filters missing)\\n\"\n",
    "            \"- 0: Incomplete or irrelevant\\n\\n\"\n",
    "    \n",
    "            \"5. Structure Similarity (0–10): Are similar joins, filters, subqueries, and groupings used as in the reference?\\n\"\n",
    "            \"- 10: Very similar\\n\"\n",
    "            \"- 6: Moderately similar\\n\"\n",
    "            \"- 2: Very different\\n\\n\"\n",
    "    \n",
    "            \"6. Readability & Maintainability (0–10): Is the SQL query easy to read, well-formatted, and maintainable?\\n\"\n",
    "            \"- 10: Excellent\\n\"\n",
    "            \"- 7: Good\\n\"\n",
    "            \"- 4: Fair\\n\"\n",
    "            \"- 1: Poor\\n\\n\"\n",
    "    \n",
    "            \"7. Overall Score (0–10): Compute using the formula below:\\n\"\n",
    "            \"   score = round(0.3 * accuracy + 0.15 * efficiency + 0.15 * (10 - hallucination) + \"\n",
    "            \"0.2 * completeness + 0.1 * structure_similarity + 0.1 * readability)\\n\\n\"\n",
    "    \n",
    "            \"Return your evaluation strictly in the following JSON format:\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \"accuracy\": integer (0–10),\\n'\n",
    "            '  \"efficiency\": integer (0–10),\\n'\n",
    "            '  \"hallucination\": integer (0–10),\\n'\n",
    "            '  \"completeness\": integer (0–10),\\n'\n",
    "            '  \"structure_similarity\": integer (0–10),\\n'\n",
    "            '  \"readability\": integer (0–10),\\n'\n",
    "            '  \"score\": integer (0–10),\\n'\n",
    "            '  \"comments\": \"Detailed feedback explaining the evaluation, including mismatches, inefficiencies, hallucinations, and suggested improvements.\"\\n'\n",
    "            \"}\"\n",
    "        )\n",
    "    )\n",
    " \n",
    "    with open(\"filtered_insurance.json\", \"r\") as file:\n",
    "        filtered_data = json.load(file)\n",
    "     \n",
    "    evaluation_results = []\n",
    "    for entry in filtered_data:\n",
    "        # print('sssssssssssssss',entry[\"question\"],'sssssssssssssssss',user_query)\n",
    "        if entry[\"question\"].strip().lower() == user_query.strip().lower():\n",
    "        # if entry[\"question\"]==user_query:\n",
    "            print('sssssssssss',user_query)\n",
    "            \n",
    "            nl_query = entry[\"question\"]\n",
    "            global ground_truth_sql\n",
    "            ground_truth_sql = entry[\"query\"]\n",
    "            conn = sqlite3.connect(DB_PATH)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            cursor.execute(ground_truth_sql)\n",
    "            db_results = cursor.fetchall()\n",
    "            \n",
    "            conn.close()\n",
    "            print('real resut&&&&&&&&&&&&&&&&&&&&&&&&&',db_results)\n",
    "    \n",
    "            \n",
    "             # Prepare the evaluation prompt for the JudgeAgent\n",
    "    evaluation_prompt = (\n",
    "                f\"Natural Language Query: {nl_query}\\n\"\n",
    "                f\"Reference SQL: {ground_truth_sql}\\n\"\n",
    "                f\"AI-Generated SQL: {generated_sql}\\n\\n\"\n",
    "                \"Please evaluate the AI-Generated SQL against the Reference SQL based on the criteria provided  and check the logic not the structure.\"\n",
    "            )\n",
    "\n",
    "            # JudgeAgent evaluates the generated SQL\n",
    "    evaluation = judge_agent.initiate_chat(\n",
    "                message=evaluation_prompt,\n",
    "                recipient=judge_agent,\n",
    "                max_turns=1\n",
    "            )\n",
    "    evaluation_results.append(evaluation.summary)\n",
    " \n",
    "    print('evaluation_results',evaluation_results[0])\n",
    "    summary = evaluation.summary   \n",
    "    print('!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!',evaluation_results[0])\n",
    "    json_str =  re.search(r\"\\{.*\\}\", evaluation_results[0], re.DOTALL)\n",
    "   \n",
    "\n",
    "    print('lllllllllllllllllllllll',json_str)\n",
    "     \n",
    "    if json_str:\n",
    "        result = json_str.group(0)\n",
    "        print('resultresultresult*****************',type(result),result)\n",
    "        try:\n",
    "            evaluation = json.loads(result)  # Now it's a dictionary\n",
    "        except json.JSONDecodeError as e:\n",
    "            return f\"<p>Error decoding JSON: {e}</p>\", \"\", \"\"\n",
    "    \n",
    "    html_table = \"\"\"\n",
    "    <h3> Evaluation Metrics (as Table)</h3>\n",
    "    <table style='width:100%; border-collapse: collapse;' border='1'>\n",
    "    <tr><th>Metric</th><th>Value</th></tr>\n",
    "    \"\"\"\n",
    "    for k, v in evaluation.items():\n",
    "        html_table += f\"<tr><td><b>{k}</b></td><td>{v}</td></tr>\"\n",
    "    html_table += \"</table>\"\n",
    "    generated_result=db_results\n",
    "    print(generated_result, db_results,'ssssssssssssssssssssssssssss')\n",
    "\n",
    "\n",
    "    \n",
    "    return html_table,ground_truth_sql,generated_sql , generated_result, db_results  \n",
    "\n",
    "\n",
    "\n",
    "def result_formating_agent(generated_result):\n",
    "    result_formatting_agent = autogen.ConversableAgent(\n",
    "        name=\"ResultFormattingAgent\",\n",
    "        llm_config=config_list,\n",
    "       system_message = (\n",
    "         \"You are responsible for formatting database query results into a structured, human-readable format. \"\n",
    "        \"When provided with raw query results, process them into a clear tabular format, numbered list, or summary, depending on the result type. \"\n",
    "        \"For large datasets, summarize key trends instead of listing every record. \"\n",
    "        \"Return the formatted output in a way that is easy to understand.\"\n",
    "        \"Once you have the final result, output the result and finally close the conversation by saying thank you\"\n",
    ")\n",
    "    )\n",
    "\n",
    "    input_to_agent = f\"generated_database = {generated_result}\"\n",
    "\n",
    "    response_user = result_formatting_agent.initiate_chat(\n",
    "    recipient=result_formatting_agent,\n",
    "    message=input_to_agent,\n",
    "    max_turns=1\n",
    "    )\n",
    "\n",
    "    \n",
    "    response_text = response_user.summary \n",
    "    print('**************', response_text)\n",
    "    return response_text,'result_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8594d1ca-6cfe-4bc3-88e2-964c7f997dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query= \"Among all the claims, which claims have a claimed amount larger than the average? List the date the claim was made and the date it was settled.\",\n",
    "DB_PATH = \"spider_db/spider_db.db\"\n",
    "\n",
    "\n",
    "# final_result,ground_truth,generated_sql,var1,var2 =final_function(DB_PATH,user_query)\n",
    "# print('kkkkkkkkkkkkkkkkkk', final_result,'\\n\\n\\n',ground_truth ,'\\n\\n\\n',generated_sql )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3eba5e01-432b-4bef-b6fe-4710f18cd7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_all():\n",
    "    return \"\", \"\", \"\", \"\", \"\", \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9709cf6-549e-4c84-90c4-7e83d2421999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://88a7fdd04be3667625.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://88a7fdd04be3667625.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "Analyze the user query : Among all the claims, which claims have a claimed amount larger than the average? List the date the claim was made and the date it was settled. \n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SchemaAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSchemaAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_xhqb): FetchingSchemaSchemaAgent *****\u001b[0m\n",
      "Arguments: \n",
      "{\"DB_PATH\": \"spider_db/spider_db.db\"}\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SchemaAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION FetchingSchemaSchemaAgent...\n",
      "Call ID: call_xhqb\n",
      "Input arguments: {'DB_PATH': 'spider_db/spider_db.db'}\u001b[0m\n",
      "\u001b[33mSchemaAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_xhqb) *****\u001b[0m\n",
      "Table: Customers\n",
      "Columns:\n",
      "  - Customer_ID (INTEGER) | Samples: 1, 2, 3\n",
      "  - Customer_Details (VARCHAR(255)) | Samples: America Jaskolski, Ellsworth Paucek, Mrs. Hanna Willms\n",
      "Table: Customer_Policies\n",
      "Columns:\n",
      "  - Policy_ID (INTEGER) | Samples: 119, 141, 143\n",
      "  - Customer_ID (INTEGER) | Samples: 1, 2, 3\n",
      "  - Policy_Type_Code (CHAR(15)) | Samples: Car, Life, Car\n",
      "  - Start_Date (DATE) | Samples: 2018-01-21, 2017-08-21, 2017-06-16\n",
      "  - End_Date (DATE) | Samples: 2017-12-15, 2017-09-29, 2017-12-09\n",
      "Table: Claims\n",
      "Columns:\n",
      "  - Claim_ID (INTEGER) | Samples: 143, 423, 442\n",
      "  - Policy_ID (INTEGER) | Samples: 744, 552, 473\n",
      "  - Date_Claim_Made (DATE) | Samples: 2017-03-11, 2016-08-12, 2017-02-24\n",
      "  - Date_Claim_Settled (DATE) | Samples: 2017-11-03, 2018-01-27, 2018-01-21\n",
      "  - Amount_Claimed (INTEGER) | Samples: 43884, 79134, 70088\n",
      "  - Amount_Settled (INTEGER) | Samples: 1085, 1724, 1189\n",
      "Table: Settlements\n",
      "Columns:\n",
      "  - Settlement_ID (INTEGER) | Samples: 357, 412, 476\n",
      "  - Claim_ID (INTEGER) | Samples: 486, 621, 801\n",
      "  - Date_Claim_Made (DATE) | Samples: 2018-08-07, 2017-08-27, 2016-09-05\n",
      "  - Date_Claim_Settled (DATE) | Samples: 2018-01-16, 2018-02-04, 2018-03-02\n",
      "  - Amount_Claimed (INTEGER) | Samples: 38543, 57669, 30954\n",
      "  - Amount_Settled (INTEGER) | Samples: 1181, 1427, 1805\n",
      "  - Customer_Policy_ID (INTEGER) | Samples: 515, 617, 943\n",
      "Table: Payments\n",
      "Columns:\n",
      "  - Payment_ID (INTEGER) | Samples: 384, 435, 484\n",
      "  - Settlement_ID (INTEGER) | Samples: 516, 476, 516\n",
      "  - Payment_Method_Code (VARCHAR(255)) | Samples: MasterCard, MasterCard, MasterCard\n",
      "  - Date_Payment_Made (DATE) | Samples: 2018-02-16, 2017-05-28, 2017-06-24\n",
      "  - Amount_Payment (INTEGER) | Samples: 241730, 448613, 456098\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SQLAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSQLAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_86kw): ValidationFromSQLAgent *****\u001b[0m\n",
      "Arguments: \n",
      "{\"db_path\": \"spider_db/spider_db.db\", \"sql_query\": \"SELECT Date_Claim_Made, Date_Claim_Settled FROM Claims WHERE Amount_Claimed > (SELECT AVG(Amount_Claimed) FROM Claims)\"}\n",
      "\u001b[32m*******************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: ValidationAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION ValidationFromSQLAgent...\n",
      "Call ID: call_86kw\n",
      "Input arguments: {'db_path': 'spider_db/spider_db.db', 'sql_query': 'SELECT Date_Claim_Made, Date_Claim_Settled FROM Claims WHERE Amount_Claimed > (SELECT AVG(Amount_Claimed) FROM Claims)'}\u001b[0m\n",
      "\u001b[33mValidationAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_86kw) *****\u001b[0m\n",
      "success\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (bc515709-5aef-4bf8-9419-6be34d87369b): Maximum rounds (5) reached\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_e2y8): FetchingSchemaUserProxy *****\u001b[0m\n",
      "Arguments: \n",
      "{\"DB_PATH\": \"spider_db/spider_db.db\"}\n",
      "\u001b[32m********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Admin\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (58ce9f78-a53b-4993-b173-7c4fb64000f6): Maximum number of consecutive auto-replies reached\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (ac8c3310-e0ce-4f89-aecc-57f2f1495c3c): No reply generated\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION FetchingSchemaUserProxy...\n",
      "Call ID: call_e2y8\n",
      "Input arguments: {'DB_PATH': 'spider_db/spider_db.db'}\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_e2y8) *****\u001b[0m\n",
      "Table: Customers\n",
      "Columns:\n",
      "  - Customer_ID (INTEGER) | Samples: 1, 2, 3\n",
      "  - Customer_Details (VARCHAR(255)) | Samples: America Jaskolski, Ellsworth Paucek, Mrs. Hanna Willms\n",
      "Table: Customer_Policies\n",
      "Columns:\n",
      "  - Policy_ID (INTEGER) | Samples: 119, 141, 143\n",
      "  - Customer_ID (INTEGER) | Samples: 1, 2, 3\n",
      "  - Policy_Type_Code (CHAR(15)) | Samples: Car, Life, Car\n",
      "  - Start_Date (DATE) | Samples: 2018-01-21, 2017-08-21, 2017-06-16\n",
      "  - End_Date (DATE) | Samples: 2017-12-15, 2017-09-29, 2017-12-09\n",
      "Table: Claims\n",
      "Columns:\n",
      "  - Claim_ID (INTEGER) | Samples: 143, 423, 442\n",
      "  - Policy_ID (INTEGER) | Samples: 744, 552, 473\n",
      "  - Date_Claim_Made (DATE) | Samples: 2017-03-11, 2016-08-12, 2017-02-24\n",
      "  - Date_Claim_Settled (DATE) | Samples: 2017-11-03, 2018-01-27, 2018-01-21\n",
      "  - Amount_Claimed (INTEGER) | Samples: 43884, 79134, 70088\n",
      "  - Amount_Settled (INTEGER) | Samples: 1085, 1724, 1189\n",
      "Table: Settlements\n",
      "Columns:\n",
      "  - Settlement_ID (INTEGER) | Samples: 357, 412, 476\n",
      "  - Claim_ID (INTEGER) | Samples: 486, 621, 801\n",
      "  - Date_Claim_Made (DATE) | Samples: 2018-08-07, 2017-08-27, 2016-09-05\n",
      "  - Date_Claim_Settled (DATE) | Samples: 2018-01-16, 2018-02-04, 2018-03-02\n",
      "  - Amount_Claimed (INTEGER) | Samples: 38543, 57669, 30954\n",
      "  - Amount_Settled (INTEGER) | Samples: 1181, 1427, 1805\n",
      "  - Customer_Policy_ID (INTEGER) | Samples: 515, 617, 943\n",
      "Table: Payments\n",
      "Columns:\n",
      "  - Payment_ID (INTEGER) | Samples: 384, 435, 484\n",
      "  - Settlement_ID (INTEGER) | Samples: 516, 476, 516\n",
      "  - Payment_Method_Code (VARCHAR(255)) | Samples: MasterCard, MasterCard, MasterCard\n",
      "  - Date_Payment_Made (DATE) | Samples: 2018-02-16, 2017-05-28, 2017-06-24\n",
      "  - Amount_Payment (INTEGER) | Samples: 241730, 448613, 456098\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SchemaAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSchemaAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "terminate\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SQLAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSQLAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "The final SQL query to answer the user's request is:\n",
      "\n",
      "SELECT Date_Claim_Made, Date_Claim_Settled \n",
      "FROM Claims \n",
      "WHERE Amount_Claimed > (SELECT AVG(Amount_Claimed) FROM Claims)\n",
      "\n",
      "This query will return the date the claim was made and the date it was settled for all claims that have a claimed amount larger than the average.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: ValidationAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mValidationAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_hr6x): ValidationToAgent *****\u001b[0m\n",
      "Arguments: \n",
      "{\"db_path\": \"spider_db/spider_db.db\", \"sql_query\": \"SELECT Date_Claim_Made, Date_Claim_Settled FROM Claims WHERE Amount_Claimed > (SELECT AVG(Amount_Claimed) FROM Claims)\"}\n",
      "\u001b[32m**************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: ValidationAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION ValidationToAgent...\n",
      "Call ID: call_hr6x\n",
      "Input arguments: {'db_path': 'spider_db/spider_db.db', 'sql_query': 'SELECT Date_Claim_Made, Date_Claim_Settled FROM Claims WHERE Amount_Claimed > (SELECT AVG(Amount_Claimed) FROM Claims)'}\u001b[0m\n",
      "\u001b[33mValidationAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_hr6x) *****\u001b[0m\n",
      "[('2016-08-12', '2018-01-27'), ('2017-02-24', '2018-01-21'), ('2018-06-14', '2017-12-20'), ('2017-08-03', '2018-02-18'), ('2016-08-26', '2017-09-04'), ('2018-07-13', '2017-11-22')]\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (43cc7fef-2274-464d-9e21-fda45519ad38): Maximum rounds (5) reached\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (52d9189a-3851-4ba6-bf48-dd7db5506968): Maximum turns (3) reached\u001b[0m\n",
      "000000000000000000000000000\n",
      "000000000000000000000000000\n",
      "000000000000000000000000000\n",
      "Extracted SQL Query:\n",
      " SELECT Date_Claim_Made, Date_Claim_Settled FROM Claims WHERE Amount_Claimed > (SELECT AVG(Amount_Claimed) FROM Claims)\n",
      "000000000000000000000000000\n",
      "000000000000000000000000000\n",
      "11111111111111111111111111111msg {'content': 'success', 'tool_responses': [{'tool_call_id': 'call_86kw', 'role': 'tool', 'content': 'success'}], 'role': 'tool', 'name': 'ValidationAgent'}\n",
      "000000000000000000000000000\n",
      "000000000000000000000000000\n",
      "000000000000000000000000000\n",
      "000000000000000000000000000\n",
      "000000000000000000000000000\n",
      "11111111111111111111111111111msg {'tool_calls': [{'id': 'call_hr6x', 'function': {'arguments': '{\"db_path\": \"spider_db/spider_db.db\", \"sql_query\": \"SELECT Date_Claim_Made, Date_Claim_Settled FROM Claims WHERE Amount_Claimed > (SELECT AVG(Amount_Claimed) FROM Claims)\"}', 'name': 'ValidationToAgent'}, 'type': 'function'}], 'content': 'None', 'role': 'assistant', 'name': 'ValidationAgent'}\n",
      "000000000000000000000000000\n",
      "11111111111111111111111111111msg {'content': \"[('2016-08-12', '2018-01-27'), ('2017-02-24', '2018-01-21'), ('2018-06-14', '2017-12-20'), ('2017-08-03', '2018-02-18'), ('2016-08-26', '2017-09-04'), ('2018-07-13', '2017-11-22')]\", 'tool_responses': [{'tool_call_id': 'call_hr6x', 'role': 'tool', 'content': \"[('2016-08-12', '2018-01-27'), ('2017-02-24', '2018-01-21'), ('2018-06-14', '2017-12-20'), ('2017-08-03', '2018-02-18'), ('2016-08-26', '2017-09-04'), ('2018-07-13', '2017-11-22')]\"}], 'role': 'tool', 'name': 'ValidationAgent'}\n",
      "✅ Found matching message:\n",
      " [('2016-08-12', '2018-01-27'), ('2017-02-24', '2018-01-21'), ('2018-06-14', '2017-12-20'), ('2017-08-03', '2018-02-18'), ('2016-08-26', '2017-09-04'), ('2018-07-13', '2017-11-22')]\n",
      "sssssssssss Among all the claims, which claims have a claimed amount larger than the average? List the date the claim was made and the date it was settled. \n",
      "real resut&&&&&&&&&&&&&&&&&&&&&&&&& [('2016-08-12', '2018-01-27'), ('2017-02-24', '2018-01-21'), ('2018-06-14', '2017-12-20'), ('2017-08-03', '2018-02-18'), ('2016-08-26', '2017-09-04'), ('2018-07-13', '2017-11-22')]\n",
      "\u001b[33mJudgeAgent\u001b[0m (to JudgeAgent):\n",
      "\n",
      "Natural Language Query: Among all the claims, which claims have a claimed amount larger than the average? List the date the claim was made and the date it was settled.\n",
      "Reference SQL: SELECT Date_Claim_Made ,  Date_Claim_Settled FROM Claims WHERE Amount_Claimed  >  ( SELECT avg(Amount_Claimed) FROM Claims )\n",
      "AI-Generated SQL: SELECT Date_Claim_Made, Date_Claim_Settled FROM Claims WHERE Amount_Claimed > (SELECT AVG(Amount_Claimed) FROM Claims)\n",
      "\n",
      "Please evaluate the AI-Generated SQL against the Reference SQL based on the criteria provided  and check the logic not the structure.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mJudgeAgent\u001b[0m (to JudgeAgent):\n",
      "\n",
      "To evaluate the AI-Generated SQL against the Reference SQL based on the provided criteria, let's break down the assessment:\n",
      "\n",
      "1. **Accuracy (0–10)**: Since the AI-Generated SQL and the Reference SQL are logically identical (both select `Date_Claim_Made` and `Date_Claim_Settled` from `Claims` where `Amount_Claimed` is greater than the average `Amount_Claimed`), they will return the same results when executed on the same database. Therefore, the accuracy score is **10**.\n",
      "\n",
      "2. **Efficiency (0–10)**: Without specific details about the database or the execution time, it's challenging to give a precise efficiency score. However, both queries are structured in a way that they should perform efficiently if the database is properly indexed, particularly on the `Amount_Claimed` column. Assuming optimal conditions and indexing, the efficiency score could be considered **10** for both queries, as they are identical and should execute in under 1 second on a moderately sized database.\n",
      "\n",
      "3. **Hallucination (0–10)**: There are no invalid elements like non-existent tables, columns, or illogical clauses in the AI-Generated SQL compared to the Reference SQL. Therefore, the hallucination score is **0**.\n",
      "\n",
      "4. **Completeness (0–10)**: The AI-Generated SQL fully answers the user’s question, which is to list the claims with amounts larger than the average, including the date the claim was made and the date it was settled. Thus, the completeness score is **10**.\n",
      "\n",
      "5. **Structure Similarity (0–10)**: The structure of the AI-Generated SQL is nearly identical to the Reference SQL. The only difference is the use of `AVG` (in all caps) in the AI-Generated SQL versus `avg` (in lowercase) in the Reference SQL, which does not affect the query's logic or execution. Therefore, the structure similarity score is **10**.\n",
      "\n",
      "6. **Readability & Maintainability (0–10)**: Both queries are simple, easy to read, and well-structured, making them maintainable. The use of standard SQL functions like `AVG` or `avg` for averaging does not impact readability. Therefore, the readability and maintainability score is **10**.\n",
      "\n",
      "7. **Overall Score (0–10)**: To calculate the overall score, we use the formula provided:\n",
      "   - score = round(0.3 * accuracy + 0.15 * efficiency + 0.15 * (10 - hallucination) + 0.2 * completeness + 0.1 * structure_similarity + 0.1 * readability)\n",
      "   - score = round(0.3 * 10 + 0.15 * 10 + 0.15 * (10 - 0) + 0.2 * 10 + 0.1 * 10 + 0.1 * 10)\n",
      "   - score = round(3 + 1.5 + 1.5 + 2 + 1 + 1)\n",
      "   - score = round(10)\n",
      "   - score = **10**\n",
      "\n",
      "Given the evaluation, the response in the requested JSON format is:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"accuracy\": 10,\n",
      "  \"efficiency\": 10,\n",
      "  \"hallucination\": 0,\n",
      "  \"completeness\": 10,\n",
      "  \"structure_similarity\": 10,\n",
      "  \"readability\": 10,\n",
      "  \"score\": 10,\n",
      "  \"comments\": \"The AI-Generated SQL is logically identical to the Reference SQL, returning the same results and using the same efficient structure. There are no hallucinations, and the query is complete, very similar in structure, and highly readable and maintainable. The only minor difference is the case used for the 'avg' function, which does not affect the query's execution or logic.\"\n",
      "}\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (62890247-ef24-4b1b-9bbf-8a97c2c7e233): Maximum turns (1) reached\u001b[0m\n",
      "evaluation_results To evaluate the AI-Generated SQL against the Reference SQL based on the provided criteria, let's break down the assessment:\n",
      "\n",
      "1. **Accuracy (0–10)**: Since the AI-Generated SQL and the Reference SQL are logically identical (both select `Date_Claim_Made` and `Date_Claim_Settled` from `Claims` where `Amount_Claimed` is greater than the average `Amount_Claimed`), they will return the same results when executed on the same database. Therefore, the accuracy score is **10**.\n",
      "\n",
      "2. **Efficiency (0–10)**: Without specific details about the database or the execution time, it's challenging to give a precise efficiency score. However, both queries are structured in a way that they should perform efficiently if the database is properly indexed, particularly on the `Amount_Claimed` column. Assuming optimal conditions and indexing, the efficiency score could be considered **10** for both queries, as they are identical and should execute in under 1 second on a moderately sized database.\n",
      "\n",
      "3. **Hallucination (0–10)**: There are no invalid elements like non-existent tables, columns, or illogical clauses in the AI-Generated SQL compared to the Reference SQL. Therefore, the hallucination score is **0**.\n",
      "\n",
      "4. **Completeness (0–10)**: The AI-Generated SQL fully answers the user’s question, which is to list the claims with amounts larger than the average, including the date the claim was made and the date it was settled. Thus, the completeness score is **10**.\n",
      "\n",
      "5. **Structure Similarity (0–10)**: The structure of the AI-Generated SQL is nearly identical to the Reference SQL. The only difference is the use of `AVG` (in all caps) in the AI-Generated SQL versus `avg` (in lowercase) in the Reference SQL, which does not affect the query's logic or execution. Therefore, the structure similarity score is **10**.\n",
      "\n",
      "6. **Readability & Maintainability (0–10)**: Both queries are simple, easy to read, and well-structured, making them maintainable. The use of standard SQL functions like `AVG` or `avg` for averaging does not impact readability. Therefore, the readability and maintainability score is **10**.\n",
      "\n",
      "7. **Overall Score (0–10)**: To calculate the overall score, we use the formula provided:\n",
      "   - score = round(0.3 * accuracy + 0.15 * efficiency + 0.15 * (10 - hallucination) + 0.2 * completeness + 0.1 * structure_similarity + 0.1 * readability)\n",
      "   - score = round(0.3 * 10 + 0.15 * 10 + 0.15 * (10 - 0) + 0.2 * 10 + 0.1 * 10 + 0.1 * 10)\n",
      "   - score = round(3 + 1.5 + 1.5 + 2 + 1 + 1)\n",
      "   - score = round(10)\n",
      "   - score = **10**\n",
      "\n",
      "Given the evaluation, the response in the requested JSON format is:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"accuracy\": 10,\n",
      "  \"efficiency\": 10,\n",
      "  \"hallucination\": 0,\n",
      "  \"completeness\": 10,\n",
      "  \"structure_similarity\": 10,\n",
      "  \"readability\": 10,\n",
      "  \"score\": 10,\n",
      "  \"comments\": \"The AI-Generated SQL is logically identical to the Reference SQL, returning the same results and using the same efficient structure. There are no hallucinations, and the query is complete, very similar in structure, and highly readable and maintainable. The only minor difference is the case used for the 'avg' function, which does not affect the query's execution or logic.\"\n",
      "}\n",
      "```\n",
      "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! To evaluate the AI-Generated SQL against the Reference SQL based on the provided criteria, let's break down the assessment:\n",
      "\n",
      "1. **Accuracy (0–10)**: Since the AI-Generated SQL and the Reference SQL are logically identical (both select `Date_Claim_Made` and `Date_Claim_Settled` from `Claims` where `Amount_Claimed` is greater than the average `Amount_Claimed`), they will return the same results when executed on the same database. Therefore, the accuracy score is **10**.\n",
      "\n",
      "2. **Efficiency (0–10)**: Without specific details about the database or the execution time, it's challenging to give a precise efficiency score. However, both queries are structured in a way that they should perform efficiently if the database is properly indexed, particularly on the `Amount_Claimed` column. Assuming optimal conditions and indexing, the efficiency score could be considered **10** for both queries, as they are identical and should execute in under 1 second on a moderately sized database.\n",
      "\n",
      "3. **Hallucination (0–10)**: There are no invalid elements like non-existent tables, columns, or illogical clauses in the AI-Generated SQL compared to the Reference SQL. Therefore, the hallucination score is **0**.\n",
      "\n",
      "4. **Completeness (0–10)**: The AI-Generated SQL fully answers the user’s question, which is to list the claims with amounts larger than the average, including the date the claim was made and the date it was settled. Thus, the completeness score is **10**.\n",
      "\n",
      "5. **Structure Similarity (0–10)**: The structure of the AI-Generated SQL is nearly identical to the Reference SQL. The only difference is the use of `AVG` (in all caps) in the AI-Generated SQL versus `avg` (in lowercase) in the Reference SQL, which does not affect the query's logic or execution. Therefore, the structure similarity score is **10**.\n",
      "\n",
      "6. **Readability & Maintainability (0–10)**: Both queries are simple, easy to read, and well-structured, making them maintainable. The use of standard SQL functions like `AVG` or `avg` for averaging does not impact readability. Therefore, the readability and maintainability score is **10**.\n",
      "\n",
      "7. **Overall Score (0–10)**: To calculate the overall score, we use the formula provided:\n",
      "   - score = round(0.3 * accuracy + 0.15 * efficiency + 0.15 * (10 - hallucination) + 0.2 * completeness + 0.1 * structure_similarity + 0.1 * readability)\n",
      "   - score = round(0.3 * 10 + 0.15 * 10 + 0.15 * (10 - 0) + 0.2 * 10 + 0.1 * 10 + 0.1 * 10)\n",
      "   - score = round(3 + 1.5 + 1.5 + 2 + 1 + 1)\n",
      "   - score = round(10)\n",
      "   - score = **10**\n",
      "\n",
      "Given the evaluation, the response in the requested JSON format is:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"accuracy\": 10,\n",
      "  \"efficiency\": 10,\n",
      "  \"hallucination\": 0,\n",
      "  \"completeness\": 10,\n",
      "  \"structure_similarity\": 10,\n",
      "  \"readability\": 10,\n",
      "  \"score\": 10,\n",
      "  \"comments\": \"The AI-Generated SQL is logically identical to the Reference SQL, returning the same results and using the same efficient structure. There are no hallucinations, and the query is complete, very similar in structure, and highly readable and maintainable. The only minor difference is the case used for the 'avg' function, which does not affect the query's execution or logic.\"\n",
      "}\n",
      "```\n",
      "lllllllllllllllllllllll <re.Match object; span=(2608, 3150), match='{\\n  \"accuracy\": 10,\\n  \"efficiency\": 10,\\n  \"hal>\n",
      "resultresultresult***************** <class 'str'> {\n",
      "  \"accuracy\": 10,\n",
      "  \"efficiency\": 10,\n",
      "  \"hallucination\": 0,\n",
      "  \"completeness\": 10,\n",
      "  \"structure_similarity\": 10,\n",
      "  \"readability\": 10,\n",
      "  \"score\": 10,\n",
      "  \"comments\": \"The AI-Generated SQL is logically identical to the Reference SQL, returning the same results and using the same efficient structure. There are no hallucinations, and the query is complete, very similar in structure, and highly readable and maintainable. The only minor difference is the case used for the 'avg' function, which does not affect the query's execution or logic.\"\n",
      "}\n",
      "[('2016-08-12', '2018-01-27'), ('2017-02-24', '2018-01-21'), ('2018-06-14', '2017-12-20'), ('2017-08-03', '2018-02-18'), ('2016-08-26', '2017-09-04'), ('2018-07-13', '2017-11-22')] [('2016-08-12', '2018-01-27'), ('2017-02-24', '2018-01-21'), ('2018-06-14', '2017-12-20'), ('2017-08-03', '2018-02-18'), ('2016-08-26', '2017-09-04'), ('2018-07-13', '2017-11-22')] ssssssssssssssssssssssssssss\n",
      "\u001b[33mResultFormattingAgent\u001b[0m (to ResultFormattingAgent):\n",
      "\n",
      "generated_database = [('2016-08-12', '2018-01-27'), ('2017-02-24', '2018-01-21'), ('2018-06-14', '2017-12-20'), ('2017-08-03', '2018-02-18'), ('2016-08-26', '2017-09-04'), ('2018-07-13', '2017-11-22')]\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mResultFormattingAgent\u001b[0m (to ResultFormattingAgent):\n",
      "\n",
      "### Formatted Database Query Results\n",
      "\n",
      "The provided database query results contain 6 records, each with two date fields. Here is a summary of the key trends and a tabular format for better understanding:\n",
      "\n",
      "#### Tabular Format\n",
      "\n",
      "| Record # | Date 1        | Date 2        |\n",
      "|----------|---------------|---------------|\n",
      "| 1        | 2016-08-12    | 2018-01-27    |\n",
      "| 2        | 2017-02-24    | 2018-01-21    |\n",
      "| 3        | 2018-06-14    | 2017-12-20    |\n",
      "| 4        | 2017-08-03    | 2018-02-18    |\n",
      "| 5        | 2016-08-26    | 2017-09-04    |\n",
      "| 6        | 2018-07-13    | 2017-11-22    |\n",
      "\n",
      "#### Key Trends\n",
      "\n",
      "- The earliest date in the 'Date 1' field is 2016-08-12, and the latest date in the 'Date 2' field is 2018-02-18.\n",
      "- The dates in 'Date 1' range from 2016 to 2018, indicating a two-year span of entries.\n",
      "- Similarly, 'Date 2' also ranges from 2017 to 2018, suggesting that these dates are generally more recent than those in 'Date 1'.\n",
      "\n",
      "Thank you\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (bb338ea7-8816-40df-8b44-9635ada3dab3): Maximum turns (1) reached\u001b[0m\n",
      "************** ### Formatted Database Query Results\n",
      "\n",
      "The provided database query results contain 6 records, each with two date fields. Here is a summary of the key trends and a tabular format for better understanding:\n",
      "\n",
      "#### Tabular Format\n",
      "\n",
      "| Record # | Date 1        | Date 2        |\n",
      "|----------|---------------|---------------|\n",
      "| 1        | 2016-08-12    | 2018-01-27    |\n",
      "| 2        | 2017-02-24    | 2018-01-21    |\n",
      "| 3        | 2018-06-14    | 2017-12-20    |\n",
      "| 4        | 2017-08-03    | 2018-02-18    |\n",
      "| 5        | 2016-08-26    | 2017-09-04    |\n",
      "| 6        | 2018-07-13    | 2017-11-22    |\n",
      "\n",
      "#### Key Trends\n",
      "\n",
      "- The earliest date in the 'Date 1' field is 2016-08-12, and the latest date in the 'Date 2' field is 2018-02-18.\n",
      "- The dates in 'Date 1' range from 2016 to 2018, indicating a two-year span of entries.\n",
      "- Similarly, 'Date 2' also ranges from 2017 to 2018, suggesting that these dates are generally more recent than those in 'Date 1'.\n",
      "\n",
      "Thank you\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with gr.Blocks() as demo:\n",
    "\n",
    "    gr.Markdown(\"# 🧠 Natural Language to SQL Evaluator\")\n",
    "    user_input = gr.Textbox(label=\"Enter your natural language query\")\n",
    "    db_path_input = gr.Textbox(label=\"DB Path\", value=\"spider_db/spider_db.db\")\n",
    "    run_btn = gr.Button(\"Run Evaluation\")\n",
    "    generated_sql_output = gr.Textbox(label=\"Generated SQL\")\n",
    "    reference_sql_output = gr.Textbox(label=\"Reference SQL (from dataset)\")\n",
    "    judge_output =gr.HTML(label=\"Judge Evaluation (as Table)\")\n",
    "    generated_result=gr.Textbox(label=\"Generated SQL result \")\n",
    "    db_results=gr.Textbox(label=\"Reference query result\")\n",
    "    clear_btn = gr.Button(\"Clear\")\n",
    "    run_btn.click(\n",
    "        final_function,\n",
    "        inputs=[db_path_input,user_input],\n",
    "        outputs=[judge_output,reference_sql_output,generated_sql_output,generated_result, db_results]\n",
    "    )\n",
    "    \n",
    "    analyze_btn = gr.Button(\"Analyze\")\n",
    "    \n",
    "    nl_output = gr.Textbox(label=\"Natural Language Summary\")\n",
    "    chart_output = gr.Image(label=\"Generated Chart\",visible=False)\n",
    "\n",
    "    analyze_btn.click(\n",
    "        result_formating_agent,\n",
    "        inputs=[generated_result],\n",
    "        outputs=[nl_output, chart_output]\n",
    "    )\n",
    "\n",
    "    \n",
    "    clear_btn.click(\n",
    "        clear_all,\n",
    "        inputs=[],\n",
    "        outputs=[user_input, reference_sql_output, generated_sql_output, generated_result, db_results, judge_output]\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b5393-018d-4db1-9a69-e6605f43db30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023b28f-af4c-4e9d-ba5e-40592ee19dc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21da7295-5949-46a0-8e07-a5f1d6b97c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e4f552-f510-434f-8191-99480b7efae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f121162-2aa5-45a7-abd8-cadee650de12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd81241-93b0-4488-980a-e86bf13a8e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7ba7bf-961a-4291-94c4-bec4f377643d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e835b682-15bb-49b8-8074-66e7ef33d841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_function_eval(DB_PATH,user_query):\n",
    "\n",
    "    user_proxy = autogen.ConversableAgent(\n",
    "    name=\"Admin\",\n",
    "    code_execution_config=False,\n",
    "    llm_config=config_list,\n",
    "    human_input_mode=\"NEVER\", max_consecutive_auto_reply=1,\n",
    "    system_message = f\"Analyze the user query :{user_query}, and pass it to SchemaAgent for further process.\",\n",
    "    is_termination_msg=lambda msg: (\n",
    "                                        msg.get(\"content\") is not None \n",
    "                                        and \"thank you\" in msg[\"content\"].lower() \n",
    "                                        and msg[\"content\"].strip().lower() != \"thank you.\"\n",
    "                                    )\n",
    ")\n",
    "\n",
    "\n",
    "    schema_agent = autogen.ConversableAgent(\n",
    "        name=\"SchemaAgent\",\n",
    "        llm_config=config_list,\n",
    "        system_message=f\"As a database expert, your task is to inspect the database located at {DB_PATH} \"\n",
    "            f\"by internally calling the function fetch_schema({DB_PATH}). \"\n",
    "            \"Do not print or output the schema directly. Instead, analyze it to identify the structure, including table names, columns, and data types. \"\n",
    "            f\"Next, examine the user query: {user_query} and determine which tables are most relevant for answering it. \"\n",
    "            \"If no relevant tables can be identified, alert the user with the message: \"\n",
    "            \"'The query cannot be answered as no relevant tables were found in the database.' Provide a reason for this and terminate the chat by saying 'Thank you'. \"\n",
    "            \"Once the relevant tables are determined or the query is deemed unanswerable, terminate the process by saying 'terminate' \"\n",
    "            \"and return only the necessary information to the SQLAgent.\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    sql_agent = autogen.ConversableAgent(\n",
    "        name=\"SQLAgent\",\n",
    "        llm_config=config_list,\n",
    "        system_message=(\n",
    "            \"As a SQL expert, your task is to generate an accurate SQL query based on the provided database schema and user query.and column name should be in the table\"\n",
    "            \"You will receive the structured schema and the user query as input from SchemaAgent.\"\n",
    "            \"Analyze the schema to determine the relevant tables and construct an optimized SQL query that accurately answers the user's request. \"\n",
    "            \"Once you have generated the SQL query, terminate the process by saying 'terminate' and return the final SQL query.\"\n",
    "            \"If no relevant tables can be identified, alert the user with the message: 'The query cannot be answered as no relevant tables were found in the database.' Provide a reason for this and terminate the chat by saying thank you\"\n",
    "            \n",
    "        )\n",
    "    )\n",
    "    \n",
    "    validation_agent = autogen.ConversableAgent(\n",
    "        name=\"ValidationAgent\",\n",
    "        llm_config=config_list,\n",
    "        system_message=(\n",
    "            \"As a SQL validation expert, your task is to validate the SQL query generated by SQLAgent. \"\n",
    "            f\"You will receive an SQL query and the database path : {DB_PATH} as input. \"\n",
    "            f\"Use the provided helper function `validate_and_execute_sql_query(sql_query, {DB_PATH})` to check whether the query is correct. \"\n",
    "            \"If the query is valid, respond with 'SQL query is valid' and return the result to ResultFormattingAgent.\"\n",
    "            \"If the query is invalid, respond with 'SQL query is invalid' along with the error message to SQLErrorCorrectionAgent.\"\n",
    "            \"Once validation is complete, terminate the process by saying 'terminate'.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    sql_error_correction_agent = autogen.ConversableAgent(\n",
    "        name=\"SQLErrorCorrectionAgent\",\n",
    "        llm_config=config_list,\n",
    "        system_message=(\n",
    "            \"You are an expert in SQL query error correction. \"\n",
    "            \"Your task is to correct SQL query errors based on the given failure message and database schema. \"\n",
    "            \"You will receive a failed SQL query along with an error message explaining why it failed. \"\n",
    "            \"Ensure the corrected query follows proper SQL syntax and is aligned with the database schema. \"\n",
    "            \"Once you have corrected the query, return the fixed version to the ValidationAgent for validation.\"\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    result_formatting_agent = autogen.ConversableAgent(\n",
    "        name=\"ResultFormattingAgent\",\n",
    "        llm_config=config_list,\n",
    "        system_message=(\n",
    "            \"You are responsible for formatting database query results into a structured, human-readable format. \"\n",
    "            \"When provided with raw query results, process them into a clear tabular format, numbered list, or summary, depending on the result type. \"\n",
    "            \"For large datasets, summarize key trends instead of listing every record. \"\n",
    "            \"Return the formatted output in a way that is easy to understand.\"\n",
    "            \"Once you have the final result, output the result and finally close the conversation by saying thank you\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    register_function(\n",
    "    fetch_schema,\n",
    "    caller=schema_agent,\n",
    "    executor=schema_agent,\n",
    "    name=\"FetchingSchemaSchemaAgent\",\n",
    "    description=\"Get Fetch Schema\",\n",
    "    )\n",
    "    \n",
    "    register_function(\n",
    "        fetch_schema,\n",
    "        caller=user_proxy,\n",
    "        executor=user_proxy,\n",
    "        name=\"FetchingSchemaUserProxy\",\n",
    "        description=\"Get Fetch Schema\",\n",
    "    )\n",
    "    \n",
    "    register_function(\n",
    "        validate_sql_query,\n",
    "        caller=sql_agent,\n",
    "        executor=validation_agent,\n",
    "        name=\"ValidationFromSQLAgent\",\n",
    "        description=\"Validationg the query\",\n",
    "    )\n",
    "    \n",
    "    register_function(\n",
    "        validate_and_execute_sql_query,\n",
    "        caller=validation_agent,\n",
    "        executor=validation_agent,\n",
    "        name=\"ValidationToAgent\",\n",
    "        description=\"Validationg the query\",\n",
    "    )\n",
    "\n",
    "    groupchat = autogen.GroupChat(\n",
    "    agents=[user_proxy, schema_agent, sql_agent, validation_agent, sql_error_correction_agent, result_formatting_agent],\n",
    "    messages=[],\n",
    "    max_round=5,\n",
    "    allowed_or_disallowed_speaker_transitions={\n",
    "    user_proxy: [schema_agent, sql_agent, validation_agent, sql_error_correction_agent, result_formatting_agent],\n",
    "    schema_agent: [user_proxy, sql_agent],\n",
    "    sql_agent: [user_proxy, validation_agent],\n",
    "    validation_agent: [sql_error_correction_agent, result_formatting_agent],\n",
    "    sql_error_correction_agent: [user_proxy, validation_agent],\n",
    "    result_formatting_agent: [user_proxy]\n",
    "    },\n",
    "    speaker_transitions_type=\"allowed\",\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # groupchat = autogen.GroupChat(\n",
    "    #     agents=[user_proxy, schema_agent, sql_agent, validation_agent, sql_error_correction_agent, result_formatting_agent],\n",
    "    #     messages=[],\n",
    "    #     max_round=5\n",
    "    # )\n",
    "    \n",
    "    \n",
    "    manager = autogen.GroupChatManager(\n",
    "        groupchat=groupchat, llm_config={\"config_list\": config_list},\n",
    "        is_termination_msg=lambda msg: (\n",
    "        isinstance(msg, dict) and\n",
    "        msg.get(\"name\") == \"ResultFormattingAgent\" and\n",
    "        bool(msg.get(\"content\"))  # Ensure content is not empty\n",
    "    )\n",
    "    )\n",
    "    chat_result = user_proxy.initiate_chat(\n",
    "    manager,\n",
    "    max_turns=3,\n",
    "    message = f\"Analyze the user query : {user_query}\"\n",
    "    )\n",
    "\n",
    "    for idx, msg in enumerate(manager.groupchat.messages):\n",
    "        if msg.get(\"name\") == \"SQLAgent\":\n",
    "            for tool_call in msg.get(\"tool_calls\", []):\n",
    "                arguments_str = tool_call.get(\"function\", {}).get(\"arguments\", \"{}\")\n",
    "                arguments = json.loads(arguments_str)  # Convert string to dictionary\n",
    "                generated_sql = arguments.get(\"sql_query\")\n",
    "    \n",
    "                if generated_sql:  # Only print if the SQL query is not None\n",
    "                    print(\"Extracted SQL Query:\\n\", generated_sql)\n",
    "\n",
    "\n",
    "    judge_agent = autogen.ConversableAgent(\n",
    "        name=\"JudgeAgent\",\n",
    "        llm_config=config_list,\n",
    "        system_message=(\n",
    "            \"You are an expert in SQL query evaluation. Your task is to assess SQL queries generated by the SQLAgent by comparing them with reference SQL queries from a dataset. \"\n",
    "            \"For each comparison, evaluate the following aspects using the defined numeric scoring guidelines:\\n\\n\"\n",
    "    \n",
    "\n",
    "            \"1. Accuracy (0–10): Does the AI-generated SQL return the same results as the reference SQL when executed on the same database?\\n\"\n",
    "            \"- 10: Identical results\\n\"\n",
    "            \"- 5: Partial match (e.g., missing filters, subset/superset of rows)\\n\"\n",
    "            \"- 0: Incorrect or different logic\\n\\n\"\n",
    "    \n",
    "            \"2. Efficiency (0–10): Evaluate the query's performance in terms of execution time, indexing, and join optimization.\\n\"\n",
    "            \"- 10: Highly efficient (<1s, optimized with indexes and joins)\\n\"\n",
    "            \"- 6: Moderate (1–3s, room for improvement)\\n\"\n",
    "            \"- 3: Low efficiency (>3s, full scans or nested loops)\\n\\n\"\n",
    "    \n",
    "            \"3. Hallucination (0–10): Check for invalid elements like non-existent tables, columns, or illogical clauses.\\n\"\n",
    "            \"- 0: No hallucination\\n\"\n",
    "            \"- 5: Minor (1–2 issues)\\n\"\n",
    "            \"- 10: Major hallucinations (e.g., many or critical invalid references)\\n\\n\"\n",
    "    \n",
    "            \"4. Completeness (0–10): Does the query fully answer the user’s question?\\n\"\n",
    "            \"- 10: Fully complete\\n\"\n",
    "            \"- 5: Partially complete (some logic or filters missing)\\n\"\n",
    "            \"- 0: Incomplete or irrelevant\\n\\n\"\n",
    "    \n",
    "            \"5. Structure Similarity (0–10): Are similar joins, filters, subqueries, and groupings used as in the reference?\\n\"\n",
    "            \"- 10: Very similar\\n\"\n",
    "            \"- 6: Moderately similar\\n\"\n",
    "            \"- 2: Very different\\n\\n\"\n",
    "    \n",
    "            \"6. Readability & Maintainability (0–10): Is the SQL query easy to read, well-formatted, and maintainable?\\n\"\n",
    "            \"- 10: Excellent\\n\"\n",
    "            \"- 7: Good\\n\"\n",
    "            \"- 4: Fair\\n\"\n",
    "            \"- 1: Poor\\n\\n\"\n",
    "    \n",
    "            \"7. Overall Score (0–10): Compute using the formula below:\\n\"\n",
    "            \"   score = round(0.3 * accuracy + 0.15 * efficiency + 0.15 * (10 - hallucination) + \"\n",
    "            \"0.2 * completeness + 0.1 * structure_similarity + 0.1 * readability)\\n\\n\"\n",
    "    \n",
    "            \"Return your evaluation strictly in the following JSON format:\\n\"\n",
    "            \"{\\n\"\n",
    "            '  \"accuracy\": integer (0–10),\\n'\n",
    "            '  \"efficiency\": integer (0–10),\\n'\n",
    "            '  \"hallucination\": integer (0–10),\\n'\n",
    "            '  \"completeness\": integer (0–10),\\n'\n",
    "            '  \"structure_similarity\": integer (0–10),\\n'\n",
    "            '  \"readability\": integer (0–10),\\n'\n",
    "            '  \"score\": integer (0–10),\\n'\n",
    "            '  \"comments\": \"Detailed feedback explaining the evaluation, including mismatches, inefficiencies, hallucinations, and suggested improvements.\"\\n'\n",
    "            \"}\"\n",
    "        )\n",
    "    )\n",
    "    with open(\"filtered_insurance.json\", \"r\") as file:\n",
    "        filtered_data = json.load(file)\n",
    "    evaluation_results = []\n",
    "\n",
    "    if isinstance(user_query, tuple):\n",
    "        user_query = user_query[0]\n",
    "        print('user_query',user_query)\n",
    "    for entry in filtered_data:\n",
    "        # print('gggggggggg',user_query,entry[\"question\"])\n",
    "        if entry[\"question\"]==user_query:\n",
    "            print('sssssssssss',user_query)\n",
    "            nl_query = entry[\"question\"]\n",
    "            ground_truth_sql = entry[\"query\"]\n",
    "     \n",
    "            # Prepare the evaluation prompt for the JudgeAgent\n",
    "            evaluation_prompt = (\n",
    "                f\"Natural Language Query: {nl_query}\\n\"\n",
    "                f\"Reference SQL: {ground_truth_sql}\\n\"\n",
    "                f\"AI-Generated SQL: {generated_sql}\\n\\n\"\n",
    "                \"Please evaluate the AI-Generated SQL against the Reference SQL based on the criteria provided  and check the logic not the structure.\"\n",
    "            )\n",
    "        \n",
    "            # JudgeAgent evaluates the generated SQL\n",
    "            evaluation = judge_agent.initiate_chat(\n",
    "                message=evaluation_prompt,\n",
    "                recipient=judge_agent,\n",
    "                max_turns=1\n",
    "            )\n",
    "        \n",
    "            evaluation_results.append(evaluation)\n",
    "            summary = evaluation.summary\n",
    "            break\n",
    "                \n",
    "    start = summary.find(\"{\")\n",
    "    end = summary.rfind(\"}\") + 1\n",
    "    json_part = summary[start:end]\n",
    "    \n",
    "    # Step 2: Parse JSON\n",
    "    eval_data = json.loads(json_part)\n",
    "    df = pd.DataFrame(list(eval_data.items()), columns=[\"Metric\", \"Value\"])\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    \n",
    "    return df,ground_truth_sql,generated_sql  # assuming 'chat_result' is your ChatResult object\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b783aac1-62aa-4143-9506-8f48f146fea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_query= \"Among all the claims, which claims have a claimed amount larger than the average? List the date the claim was made and the date it was settled.\",\n",
    "DB_PATH = \"spider_db/spider_db.db\"\n",
    "\n",
    "\n",
    "\n",
    "# final_result,ground_truth,generated_sql =final_function_eval(DB_PATH,user_query)\n",
    "# print('kkkkkkkkkkkkkkkkkk', final_result,'\\n\\n\\n',ground_truth ,'\\n\\n\\n',generated_sql )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a09b4b6d-ccf1-477f-ae6b-c31cb3fec8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=pd.read_csv('evaluation_result_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4343ba-6f63-4d12-9eaa-963341f48d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the method, date and amount of each payment? Sort the list in ascending order of date. \n",
      "\n",
      "\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "Analyze the user query : What are the method, date and amount of each payment? Sort the list in ascending order of date.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SQLAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSQLAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_4g91): ValidationFromSQLAgent *****\u001b[0m\n",
      "Arguments: \n",
      "{\"db_path\": \"path_to_database\", \"sql_query\": \"SELECT method, date, amount FROM payments ORDER BY date ASC;\"}\n",
      "\u001b[32m*******************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: ValidationAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION ValidationFromSQLAgent...\n",
      "Call ID: call_4g91\n",
      "Input arguments: {'db_path': 'path_to_database', 'sql_query': 'SELECT method, date, amount FROM payments ORDER BY date ASC;'}\u001b[0m\n",
      "\u001b[33mValidationAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_4g91) *****\u001b[0m\n",
      "failure: no such table: payments\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SQLErrorCorrectionAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSQLErrorCorrectionAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "<function=ValidationFromSQLAgent({\"type\": \"function\", \"name\": \"validate_query\", \"parameters\": {\"query\": \"SELECT method, date, amount FROM payment ORDER BY date ASC;\"}})\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: ValidationAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mValidationAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_ns66): ValidationToAgent *****\u001b[0m\n",
      "Arguments: \n",
      "{\"db_path\": \"spider_db/spider_db.db\", \"sql_query\": \"SELECT method, date, amount FROM payment ORDER BY date ASC;\"}\n",
      "\u001b[32m**************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (79d03ae6-37a9-47e5-83ce-fbaeb0dc916d): Maximum rounds (5) reached\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_ns66) *****\u001b[0m\n",
      "Error: Function ValidationToAgent not found.\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SchemaAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSchemaAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_8pb0): FetchingSchemaSchemaAgent *****\u001b[0m\n",
      "Arguments: \n",
      "{\"DB_PATH\": \"spider_db/spider_db.db\"}\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SchemaAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION FetchingSchemaSchemaAgent...\n",
      "Call ID: call_8pb0\n",
      "Input arguments: {'DB_PATH': 'spider_db/spider_db.db'}\u001b[0m\n",
      "\u001b[33mSchemaAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_8pb0) *****\u001b[0m\n",
      "Table: Customers\n",
      "Columns:\n",
      "  - Customer_ID (INTEGER) | Samples: 1, 2, 3\n",
      "  - Customer_Details (VARCHAR(255)) | Samples: America Jaskolski, Ellsworth Paucek, Mrs. Hanna Willms\n",
      "Table: Customer_Policies\n",
      "Columns:\n",
      "  - Policy_ID (INTEGER) | Samples: 119, 141, 143\n",
      "  - Customer_ID (INTEGER) | Samples: 1, 2, 3\n",
      "  - Policy_Type_Code (CHAR(15)) | Samples: Car, Life, Car\n",
      "  - Start_Date (DATE) | Samples: 2018-01-21, 2017-08-21, 2017-06-16\n",
      "  - End_Date (DATE) | Samples: 2017-12-15, 2017-09-29, 2017-12-09\n",
      "Table: Claims\n",
      "Columns:\n",
      "  - Claim_ID (INTEGER) | Samples: 143, 423, 442\n",
      "  - Policy_ID (INTEGER) | Samples: 744, 552, 473\n",
      "  - Date_Claim_Made (DATE) | Samples: 2017-03-11, 2016-08-12, 2017-02-24\n",
      "  - Date_Claim_Settled (DATE) | Samples: 2017-11-03, 2018-01-27, 2018-01-21\n",
      "  - Amount_Claimed (INTEGER) | Samples: 43884, 79134, 70088\n",
      "  - Amount_Settled (INTEGER) | Samples: 1085, 1724, 1189\n",
      "Table: Settlements\n",
      "Columns:\n",
      "  - Settlement_ID (INTEGER) | Samples: 357, 412, 476\n",
      "  - Claim_ID (INTEGER) | Samples: 486, 621, 801\n",
      "  - Date_Claim_Made (DATE) | Samples: 2018-08-07, 2017-08-27, 2016-09-05\n",
      "  - Date_Claim_Settled (DATE) | Samples: 2018-01-16, 2018-02-04, 2018-03-02\n",
      "  - Amount_Claimed (INTEGER) | Samples: 38543, 57669, 30954\n",
      "  - Amount_Settled (INTEGER) | Samples: 1181, 1427, 1805\n",
      "  - Customer_Policy_ID (INTEGER) | Samples: 515, 617, 943\n",
      "Table: Payments\n",
      "Columns:\n",
      "  - Payment_ID (INTEGER) | Samples: 384, 435, 484\n",
      "  - Settlement_ID (INTEGER) | Samples: 516, 476, 516\n",
      "  - Payment_Method_Code (VARCHAR(255)) | Samples: MasterCard, MasterCard, MasterCard\n",
      "  - Date_Payment_Made (DATE) | Samples: 2018-02-16, 2017-05-28, 2017-06-24\n",
      "  - Amount_Payment (INTEGER) | Samples: 241730, 448613, 456098\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SQLAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSQLAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_025s): ValidationFromSQLAgent *****\u001b[0m\n",
      "Arguments: \n",
      "{\"db_path\": \"spider_db/spider_db.db\", \"sql_query\": \"SELECT Payment_Method_Code AS method, Date_Payment_Made AS date, Amount_Payment AS amount FROM Payments ORDER BY Date_Payment_Made ASC;\"}\n",
      "\u001b[32m*******************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: ValidationAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION ValidationFromSQLAgent...\n",
      "Call ID: call_025s\n",
      "Input arguments: {'db_path': 'spider_db/spider_db.db', 'sql_query': 'SELECT Payment_Method_Code AS method, Date_Payment_Made AS date, Amount_Payment AS amount FROM Payments ORDER BY Date_Payment_Made ASC;'}\u001b[0m\n",
      "\u001b[33mValidationAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_025s) *****\u001b[0m\n",
      "success\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (96549eed-ed2a-4df0-afe8-f1abb403b0de): Maximum rounds (5) reached\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (aad538cd-87ae-4744-8e95-f3adaf939344): Maximum number of consecutive auto-replies reached\u001b[0m\n",
      "Extracted SQL Query:\n",
      " SELECT method, date, amount FROM payments ORDER BY date ASC;\n",
      "Extracted SQL Query:\n",
      " SELECT Payment_Method_Code AS method, Date_Payment_Made AS date, Amount_Payment AS amount FROM Payments ORDER BY Date_Payment_Made ASC;\n",
      "sssssssssss What are the method, date and amount of each payment? Sort the list in ascending order of date.\n",
      "\u001b[33mJudgeAgent\u001b[0m (to JudgeAgent):\n",
      "\n",
      "Natural Language Query: What are the method, date and amount of each payment? Sort the list in ascending order of date.\n",
      "Reference SQL: SELECT Payment_Method_Code ,  Date_Payment_Made ,  Amount_Payment FROM Payments ORDER BY Date_Payment_Made ASC\n",
      "AI-Generated SQL: SELECT Payment_Method_Code AS method, Date_Payment_Made AS date, Amount_Payment AS amount FROM Payments ORDER BY Date_Payment_Made ASC;\n",
      "\n",
      "Please evaluate the AI-Generated SQL against the Reference SQL based on the criteria provided  and check the logic not the structure.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mJudgeAgent\u001b[0m (to JudgeAgent):\n",
      "\n",
      "To evaluate the AI-Generated SQL against the Reference SQL, we'll assess the query based on the provided criteria.\n",
      "\n",
      "1. **Accuracy (0–10)**: The AI-generated SQL returns the same columns as the reference SQL, albeit with aliases. The logic is identical, and the results will be the same when executed on the same database. **Accuracy: 10**\n",
      "\n",
      "2. **Efficiency (0–10)**: Both queries have the same structure and do not contain any joins, subqueries, or complex calculations. The performance will be similar, and since there are no optimizations mentioned, we'll assume a moderate efficiency. **Efficiency: 6**\n",
      "\n",
      "3. **Hallucination (0–10)**: The AI-generated SQL does not contain any invalid elements, such as non-existent tables, columns, or illogical clauses. **Hallucination: 0**\n",
      "\n",
      "4. **Completeness (0–10)**: The AI-generated SQL fully answers the user's question by selecting the required columns and sorting the results in ascending order by date. **Completeness: 10**\n",
      "\n",
      "5. **Structure Similarity (0–10)**: Although the AI-generated SQL uses aliases for the columns, the overall structure is very similar to the reference SQL. **Structure Similarity: 10**\n",
      "\n",
      "6. **Readability & Maintainability (0–10)**: The AI-generated SQL is well-formatted and easy to read. The use of aliases for the columns improves readability. **Readability & Maintainability: 10**\n",
      "\n",
      "7. **Overall Score (0–10)**: Using the formula, we calculate the overall score as follows:\n",
      "   score = round(0.3 * 10 + 0.15 * 6 + 0.15 * (10 - 0) + 0.2 * 10 + 0.1 * 10 + 0.1 * 10)\n",
      "   score = round(3 + 0.9 + 1.5 + 2 + 1 + 1)\n",
      "   score = round(9.4)\n",
      "   **Overall Score: 9**\n",
      "\n",
      "Here's the evaluation in the required JSON format:\n",
      "\n",
      "```\n",
      "{\n",
      "  \"accuracy\": 10,\n",
      "  \"efficiency\": 6,\n",
      "  \"hallucination\": 0,\n",
      "  \"completeness\": 10,\n",
      "  \"structure_similarity\": 10,\n",
      "  \"readability\": 10,\n",
      "  \"score\": 9,\n",
      "  \"comments\": \"The AI-generated SQL is almost identical to the reference SQL in terms of logic. It returns the same columns, sorts the results in the same order, and does not contain any hallucinations or inefficiencies. The use of aliases improves readability. The only potential improvement is optimizing the query for better performance.\"\n",
      "}\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (5ce86fc1-7826-4e77-9b4f-921f123a6bde): Maximum turns (1) reached\u001b[0m\n",
      "Among all the claims, what is the settlement amount of the claim with the largest claim amount? List both the settlement amount and claim amount. \n",
      "\n",
      "\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "Analyze the user query : Among all the claims, what is the settlement amount of the claim with the largest claim amount? List both the settlement amount and claim amount.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SQLAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSQLAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_k3cp): ValidationFromSQLAgent *****\u001b[0m\n",
      "Arguments: \n",
      "{\"db_path\": \"insurance_claims.db\", \"sql_query\": \"SELECT settlement_amount, claim_amount FROM Claims ORDER BY claim_amount DESC LIMIT 1;\"}\n",
      "\u001b[32m*******************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: ValidationAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION ValidationFromSQLAgent...\n",
      "Call ID: call_k3cp\n",
      "Input arguments: {'db_path': 'insurance_claims.db', 'sql_query': 'SELECT settlement_amount, claim_amount FROM Claims ORDER BY claim_amount DESC LIMIT 1;'}\u001b[0m\n",
      "\u001b[33mValidationAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_k3cp) *****\u001b[0m\n",
      "failure: no such table: Claims\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SQLErrorCorrectionAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSQLErrorCorrectionAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "<function=ValidationFromSQLAgent>{\"db_path\": \"insurance_claims.db\", \"sql_query\": \"SELECT settlement_amount, claim_amount FROM insurance_claims ORDER BY claim_amount DESC LIMIT 1;\"}</function>\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: ValidationAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mValidationAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_77vp): ValidationToAgent *****\u001b[0m\n",
      "Arguments: \n",
      "{\"db_path\": \"insurance_claims.db\", \"sql_query\": \"SELECT settlement_amount, claim_amount FROM insurance_claims ORDER BY claim_amount DESC LIMIT 1;\"}\n",
      "\u001b[32m**************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (6d6ea8c4-e38d-4bd0-8f00-c59154055ecf): Maximum rounds (5) reached\u001b[0m\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_77vp) *****\u001b[0m\n",
      "Error: Function ValidationToAgent not found.\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SchemaAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSchemaAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_m04f): FetchingSchemaSchemaAgent *****\u001b[0m\n",
      "Arguments: \n",
      "{\"DB_PATH\": \"spider_db/spider_db.db\"}\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SchemaAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION FetchingSchemaSchemaAgent...\n",
      "Call ID: call_m04f\n",
      "Input arguments: {'DB_PATH': 'spider_db/spider_db.db'}\u001b[0m\n",
      "\u001b[33mSchemaAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_m04f) *****\u001b[0m\n",
      "Table: Customers\n",
      "Columns:\n",
      "  - Customer_ID (INTEGER) | Samples: 1, 2, 3\n",
      "  - Customer_Details (VARCHAR(255)) | Samples: America Jaskolski, Ellsworth Paucek, Mrs. Hanna Willms\n",
      "Table: Customer_Policies\n",
      "Columns:\n",
      "  - Policy_ID (INTEGER) | Samples: 119, 141, 143\n",
      "  - Customer_ID (INTEGER) | Samples: 1, 2, 3\n",
      "  - Policy_Type_Code (CHAR(15)) | Samples: Car, Life, Car\n",
      "  - Start_Date (DATE) | Samples: 2018-01-21, 2017-08-21, 2017-06-16\n",
      "  - End_Date (DATE) | Samples: 2017-12-15, 2017-09-29, 2017-12-09\n",
      "Table: Claims\n",
      "Columns:\n",
      "  - Claim_ID (INTEGER) | Samples: 143, 423, 442\n",
      "  - Policy_ID (INTEGER) | Samples: 744, 552, 473\n",
      "  - Date_Claim_Made (DATE) | Samples: 2017-03-11, 2016-08-12, 2017-02-24\n",
      "  - Date_Claim_Settled (DATE) | Samples: 2017-11-03, 2018-01-27, 2018-01-21\n",
      "  - Amount_Claimed (INTEGER) | Samples: 43884, 79134, 70088\n",
      "  - Amount_Settled (INTEGER) | Samples: 1085, 1724, 1189\n",
      "Table: Settlements\n",
      "Columns:\n",
      "  - Settlement_ID (INTEGER) | Samples: 357, 412, 476\n",
      "  - Claim_ID (INTEGER) | Samples: 486, 621, 801\n",
      "  - Date_Claim_Made (DATE) | Samples: 2018-08-07, 2017-08-27, 2016-09-05\n",
      "  - Date_Claim_Settled (DATE) | Samples: 2018-01-16, 2018-02-04, 2018-03-02\n",
      "  - Amount_Claimed (INTEGER) | Samples: 38543, 57669, 30954\n",
      "  - Amount_Settled (INTEGER) | Samples: 1181, 1427, 1805\n",
      "  - Customer_Policy_ID (INTEGER) | Samples: 515, 617, 943\n",
      "Table: Payments\n",
      "Columns:\n",
      "  - Payment_ID (INTEGER) | Samples: 384, 435, 484\n",
      "  - Settlement_ID (INTEGER) | Samples: 516, 476, 516\n",
      "  - Payment_Method_Code (VARCHAR(255)) | Samples: MasterCard, MasterCard, MasterCard\n",
      "  - Date_Payment_Made (DATE) | Samples: 2018-02-16, 2017-05-28, 2017-06-24\n",
      "  - Amount_Payment (INTEGER) | Samples: 241730, 448613, 456098\n",
      "\u001b[32m**************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: SQLAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mSQLAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "Based on the provided schema, the relevant table for the user query is the \"Claims\" table. The \"Claims\" table contains the columns \"Amount_Claimed\" and \"Amount_Settled\", which correspond to the claim amount and settlement amount, respectively.\n",
      "\n",
      "Here is the optimized SQL query that accurately answers the user's request:\n",
      "\n",
      "```sql\n",
      "SELECT Amount_Settled, Amount_Claimed \n",
      "FROM Claims \n",
      "ORDER BY Amount_Claimed DESC \n",
      "LIMIT 1;\n",
      "```\n",
      "\n",
      "This query selects the settlement amount and claim amount from the \"Claims\" table, orders the results by the claim amount in descending order, and limits the output to the top row, which corresponds to the claim with the largest claim amount.\n",
      "\n",
      "terminate \n",
      "\n",
      "The final SQL query is: \n",
      "SELECT Amount_Settled, Amount_Claimed \n",
      "FROM Claims \n",
      "ORDER BY Amount_Claimed DESC \n",
      "LIMIT 1;\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: ValidationAgent\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mValidationAgent\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_fxkj): ValidationToAgent *****\u001b[0m\n",
      "Arguments: \n",
      "{\"db_path\": \"spider_db/spider_db.db\", \"sql_query\": \"SELECT Amount_Settled, Amount_Claimed FROM Claims ORDER BY Amount_Claimed DESC LIMIT 1;\"}\n",
      "\u001b[32m**************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (4724bd36-7326-4884-9089-3e8ec499243f): Maximum rounds (5) reached\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (1cfcc29d-23dd-4730-86a8-26611c600f68): Maximum number of consecutive auto-replies reached\u001b[0m\n",
      "Extracted SQL Query:\n",
      " SELECT settlement_amount, claim_amount FROM Claims ORDER BY claim_amount DESC LIMIT 1;\n",
      "sssssssssss Among all the claims, what is the settlement amount of the claim with the largest claim amount? List both the settlement amount and claim amount.\n",
      "\u001b[33mJudgeAgent\u001b[0m (to JudgeAgent):\n",
      "\n",
      "Natural Language Query: Among all the claims, what is the settlement amount of the claim with the largest claim amount? List both the settlement amount and claim amount.\n",
      "Reference SQL: SELECT Amount_Settled ,  Amount_Claimed FROM Claims ORDER BY Amount_Claimed DESC LIMIT 1\n",
      "AI-Generated SQL: SELECT settlement_amount, claim_amount FROM Claims ORDER BY claim_amount DESC LIMIT 1;\n",
      "\n",
      "Please evaluate the AI-Generated SQL against the Reference SQL based on the criteria provided  and check the logic not the structure.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mJudgeAgent\u001b[0m (to JudgeAgent):\n",
      "\n",
      "To evaluate the AI-Generated SQL against the Reference SQL, let's analyze the query based on the provided criteria:\n",
      "\n",
      "1. **Accuracy (0–10)**: \n",
      "The AI-Generated SQL and the Reference SQL are trying to achieve the same logical result, which is to find the settlement amount and claim amount of the claim with the largest claim amount. However, the column names in the AI-Generated SQL (`settlement_amount`, `claim_amount`) do not match the column names in the Reference SQL (`Amount_Settled`, `Amount_Claimed`). Assuming that `settlement_amount` corresponds to `Amount_Settled` and `claim_amount` corresponds to `Amount_Claimed`, the logic is identical. Therefore, the accuracy is 10.\n",
      "\n",
      "2. **Efficiency (0–10)**: \n",
      "Both queries have the same structure in terms of ordering and limiting the results, which suggests similar execution plans. The efficiency of these queries depends on the indexing of the `claim_amount` (or `Amount_Claimed`) column. If this column is indexed, both queries can be considered highly efficient, with execution times potentially less than 1 second. Assuming optimal indexing, the efficiency score is 10.\n",
      "\n",
      "3. **Hallucination (0–10)**: \n",
      "The AI-Generated SQL uses `settlement_amount` and `claim_amount`, which are not the actual column names in the reference query. However, assuming these are aliases or actual column names in a different schema, the query does not contain any illogical clauses or references to non-existent tables. But, given the column name mismatch, there's a minor issue here, suggesting a score of 5 for minor hallucination due to the potential misunderstanding of column names.\n",
      "\n",
      "4. **Completeness (0–10)**: \n",
      "The AI-Generated SQL fully answers the user’s question by selecting both the settlement amount and the claim amount for the claim with the largest claim amount. Therefore, the completeness score is 10.\n",
      "\n",
      "5. **Structure Similarity (0–10)**: \n",
      "Despite the difference in column names, the structure of both queries in terms of sorting and limiting is identical. However, due to the column name discrepancies, it's not a perfect match. The structure similarity score, focusing on the logic rather than the exact syntax or naming, is 10 because both queries logically achieve the same result.\n",
      "\n",
      "6. **Readability & Maintainability (0–10)**: \n",
      "The AI-Generated SQL is easy to read and well-formatted. The use of descriptive column names (`settlement_amount`, `claim_amount`) improves readability, assuming these names are standard in the context. Therefore, the readability and maintainability score is 10.\n",
      "\n",
      "7. **Overall Score (0–10)**: \n",
      "Using the formula: \n",
      "score = round(0.3 * accuracy + 0.15 * efficiency + 0.15 * (10 - hallucination) + 0.2 * completeness + 0.1 * structure_similarity + 0.1 * readability)\n",
      "score = round(0.3 * 10 + 0.15 * 10 + 0.15 * (10 - 5) + 0.2 * 10 + 0.1 * 10 + 0.1 * 10)\n",
      "score = round(3 + 1.5 + 0.15 * 5 + 2 + 1 + 1)\n",
      "score = round(3 + 1.5 + 0.75 + 2 + 1 + 1)\n",
      "score = round(9.25)\n",
      "score = 9\n",
      "\n",
      "Here's the evaluation in JSON format:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"accuracy\": 10,\n",
      "  \"efficiency\": 10,\n",
      "  \"hallucination\": 5,\n",
      "  \"completeness\": 10,\n",
      "  \"structure_similarity\": 10,\n",
      "  \"readability\": 10,\n",
      "  \"score\": 9,\n",
      "  \"comments\": \"The AI-Generated SQL achieves the same logical result as the Reference SQL, despite differences in column names. It is efficient, complete, and highly readable. The minor hallucination is due to the potential misuse of column names. Overall, the query effectively answers the user's question but might require adjustment to match the actual database schema.\"\n",
      "}\n",
      "```\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (2ce29b1a-285a-4bfa-b6d8-cd8ae700bf73): Maximum turns (1) reached\u001b[0m\n",
      "Find the settlement amount of the claim with the largest claim amount. Show both the settlement amount and claim amount. \n",
      "\n",
      "\n",
      "\u001b[33mAdmin\u001b[0m (to chat_manager):\n",
      "\n",
      "Analyze the user query : Find the settlement amount of the claim with the largest claim amount. Show both the settlement amount and claim amount.\n"
     ]
    }
   ],
   "source": [
    "output_file = \"final_results_eval.txt\"\n",
    "\n",
    "with open(output_file, \"w\",) as f:\n",
    "    # Iterate through the first four questions in the DataFrame\n",
    "    for question in df_new['question'][5:10]:\n",
    "        # Print the question to the console\n",
    "        print(question, '\\n\\n')\n",
    "        \n",
    "        # Evaluate the query using your function\n",
    "        final_result, ground_truth, generated_sql = final_function_eval(DB_PATH, question)\n",
    "        \n",
    "        # Prepare the output string\n",
    "        output_str = (\n",
    "            f\"Question: {question}\\n\\n\"\n",
    "            f\"Final Result:\\n{final_result}\\n\\n\"\n",
    "            f\"Ground Truth:\\n{ground_truth}\\n\\n\"\n",
    "            f\"Generated SQL:\\n{generated_sql}\\n\\n\"\n",
    "            + \"-\" * 50 + \"\\n\\n\"\n",
    "        )\n",
    "        \n",
    "        # Write the output to the file\n",
    "        f.write(output_str)\n",
    "        \n",
    "print(f\"Results stored in {output_file}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008a632e-acf7-4a91-b49e-3b14e7765f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('filtered_insurance.json', 'r') as reference:\n",
    "    data = json.load(reference)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "main_data=df[['db_id','query','question']]\n",
    "main_data['question'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c6b27a-9d66-49c3-b3f9-3e8e4a44f4db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d0ddbe-1928-4c3a-83ac-99c01ca19e1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221c90a2-4889-4e03-9528-5ffcd89b448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('filtered_insurance.json', 'r') as reference:\n",
    "    data = json.load(reference)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "main_data=df[['db_id','query','question']]\n",
    "len(main_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29071ab-6458-413e-a6c8-94b81aae53d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv_new)",
   "language": "python",
   "name": "myenv_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
